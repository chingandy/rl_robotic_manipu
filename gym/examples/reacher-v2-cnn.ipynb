{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reacher-v2 Environment with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('Reacher-v101')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(49152,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49152,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "state size:  49152\n",
      "WARNING:tensorflow:From /Users/chingandywu/master-thesis/code/.venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, load_model\n",
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "EPISODES = 1000 #Maximum number of episodes\n",
    "\n",
    "#DQN Agent for the reacher-v2\n",
    "#Q function approximation with NN, experience replay, and target network\n",
    "class DQNAgent:\n",
    "    #Constructor for the agent (invoked when DQN is first called in main)\n",
    "    def __init__(self, state_size, action_space):\n",
    "        self.check_solve = False\t#If True, stop if you satisfy solution condition\n",
    "        self.render = False #If you want to see Cartpole learning, then change to True\n",
    "        self.action_space = action_space\n",
    "        #Get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = len(action_space)\n",
    "        \n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "        #Set hyper parameters for the DQN. Do not adjust those labeled as Fixed.\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.005  # 0.005\n",
    "        self.epsilon = 0.02 #Fixed\n",
    "        self.batch_size = 32 #Fixed\n",
    "        self.memory_size = 500000  # 1000\n",
    "        self.train_start = 1000 #Fixed\n",
    "        self.target_update_frequency = 1\n",
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "        #Number of test states for Q value plots\n",
    "        self.test_state_no =10000 # 10000\n",
    "\n",
    "        #Create memory buffer using deque\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "\n",
    "        #Create main network and target network (using build_model defined below)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        #Initialize target network\n",
    "        self.update_target_model()\n",
    "\n",
    "    #Approximate Q function using Neural Network\n",
    "    #State is the input and the Q Values are the output.\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "        #Edit the Neural Network model here\n",
    "        #Tip: Consult https://keras.io/getting-started/sequential-model-guide/\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4096, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='relu'))\n",
    "        #\n",
    "        # self.model = Sequential()\n",
    "        # self.model.add(Dense(16, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        # self.model.add(Dense(16, activation=\"relu\"))\n",
    "        # self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        # self.model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "#     set_trace()\n",
    "    #After some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #Get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "        #Insert your e-greedy policy code here\n",
    "        #Tip 1: Use the random package to generate a random action.\n",
    "        #Tip 2: Use keras.model.predict() to compute Q-values from the state.\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action =  random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            action =  np.argmax(q_value[0])\n",
    "        # action = random.randrange(self.action_size)\n",
    "        return action\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "    #Save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) #Add sample to the end of the list\n",
    "\n",
    "    #Sample <s,a,r,s'> from replay memory\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start: #Do not train if not enough memory\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory)) #Train on at most as many samples as you have in memory\n",
    "        mini_batch = random.sample(self.memory, batch_size) #Uniformly sample the memory buffer\n",
    "        #Preallocate network and target network input matrices.\n",
    "        update_input = np.zeros((batch_size, self.state_size)) #batch_size by state_size two-dimensional array (not matrix!)\n",
    "        update_target = np.zeros((batch_size, self.state_size)) #Same as above, but used for the target network\n",
    "        action, reward, done = [], [], [] #Empty arrays that will grow dynamically\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0] #Allocate s(i) to the network input array from iteration i in the batch\n",
    "            action.append(mini_batch[i][1]) #Store a(i)\n",
    "            reward.append(mini_batch[i][2]) #Store r(i)\n",
    "            update_target[i] = mini_batch[i][3] #Allocate s'(i) for the target network array from iteration i in the batch\n",
    "            done.append(mini_batch[i][4])  #Store done(i)\n",
    "\n",
    "        target = self.model.predict(update_input) #Generate target values for training the inner loop network using the network model\n",
    "        target_val = self.target_model.predict(update_target) #Generate the target values for training the outer loop target network\n",
    "        #Q Learning: get maximum Q value at s' from target network\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "        #Insert your Q-learning code here\n",
    "        #Tip 1: Observe that the Q-values are stored in the variable target\n",
    "        #Tip 2: What is the Q-value of the action taken at the last state of the episode?\n",
    "        for i in range(self.batch_size): #For every batch\n",
    "            # target[i][action[i]] = random.randint(0,1)\n",
    "            ############################################################### edited by andy\n",
    "#             action2ind = {(-0.0001, 0):0,(0.0001, 0):1,(0 , -0.0001):2,(0, 0.0001):3}\n",
    "#             action2ind = {(-0.0001, 0):0, (-0.01, 0):1,(0.0001, 0):2, (0.01, 0):3,(0 , -0.0001):4, (0, -0.01):5,(0, 0.0001):6, (0, 0.01):7}\n",
    "            \n",
    "#             action_tuple = tuple(action[i])\n",
    "#             action_ind = action2ind[action_tuple]\n",
    "            action_ind = self.action_space.index(action[i])\n",
    "            if done[i]:\n",
    "                target[i][action_ind]= reward[i]\n",
    "            else:\n",
    "                target[i][action_ind] = reward[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "            #################################################################\n",
    "#             if done[i]:\n",
    "#                 target[i][action[i]]= reward[i]\n",
    "#             else:\n",
    "#                 target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "#                     np.amax(target_val[i]))\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "        #Train the inner loop network\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_model(self, path_to_model, path_to_target):\n",
    "            self.model.save(path_to_model)\n",
    "            self.target_model.save(path_to_target)\n",
    "            return\n",
    "        \n",
    "    def restore_model(self, path_to_model, path_to_target):\n",
    "            self.model = load_model(path_to_model)\n",
    "            self.target_model = load_model(path_to_target)\n",
    "            return\n",
    "        \n",
    "    #Plots the score per episode as well as the maximum q value per episode, averaged over precollected states.\n",
    "    def plot_data(self, episodes, scores, max_q_mean):\n",
    "        pylab.figure(0)\n",
    "        pylab.plot(episodes, max_q_mean, 'b')\n",
    "        pylab.xlabel(\"Episodes\")\n",
    "        pylab.ylabel(\"Average Q Value\")\n",
    "        pylab.savefig(\"qvalues.png\")\n",
    "\n",
    "        pylab.figure(1)\n",
    "        pylab.plot(episodes, scores, 'b')\n",
    "        pylab.xlabel(\"Episodes\")\n",
    "        pylab.ylabel(\"Score\")\n",
    "        pylab.savefig(\"scores.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('Reacher-v101') # Reacher-v101 environment is the edited version of Reacher-v0 adapted for CNN\n",
    "    #Get state and action sizes from the environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    print(\"state size: \", state_size)\n",
    "    action_size = len(env.action_space)\n",
    "#     action_size = env.action_space.n\n",
    "    #Create agent, see the DQNAgent __init__ method for details\n",
    "#     set_trace()\n",
    "    agent = DQNAgent(state_size, env.action_space)\n",
    "    \n",
    "    # load the pre-trained model\n",
    "    path_to_model = 'model_cnn.h5'\n",
    "    path_to_target = 'target_model_cnn.h5'\n",
    "    if os.path.isfile(path_to_model) and os.path.isfile(path_to_target):\n",
    "        print(\"Loading the pre-trained model......\")\n",
    "        agent.restore_model(path_to_model, path_to_target)\n",
    "    else:\n",
    "        print(\"Pre-trained model doesn't exist.\")\n",
    "    \n",
    "\n",
    "    # Collect test states for plotting Q values using uniform random policy\n",
    "    test_states = np.zeros((agent.test_state_no, state_size))\n",
    "    max_q = np.zeros((EPISODES, agent.test_state_no))\n",
    "    max_q_mean = np.zeros((EPISODES,1))\n",
    "    \n",
    "    done = True\n",
    "    for i in range(agent.test_state_no):\n",
    "        if done:\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            test_states[i] = state\n",
    "        else:\n",
    "            #############################\n",
    "#             action = random.randrange(action_size)\n",
    "\n",
    "            action_idx = random.randrange(action_size)\n",
    "            action = env.action_space[action_idx]\n",
    "            ###################################\n",
    "#             if done:\n",
    "#                 print(\"Before Done: \", done)\n",
    "            next_state, reward, done, info= env.step(action)\n",
    "#             if done:\n",
    "#                 print(\"Done: \", done)\n",
    "#                 print(\"Info: \", info)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            test_states[i] = state\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    scores, episodes = [], [] #Create dynamically growing score and episode counters\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset() #Initialize/reset the environment\n",
    "        state = np.reshape(state, [1, state_size]) #Reshape state so that to a 1 by state_size two-dimensional array ie. [x_1,x_2] to [[x_1,x_2]]\n",
    "        #Compute Q values for plotting\n",
    "        tmp = agent.model.predict(test_states)  # tmp.shape = 10000 * 4\n",
    "        max_q[e][:] = np.max(tmp, axis=1)\n",
    "        max_q_mean[e] = np.mean(max_q[e][:])\n",
    "        count = 0 \n",
    "        while not done:\n",
    "#             if count % 10 == 0:\n",
    "#                 print(\"counter: \", count)\n",
    "#             count += 1\n",
    "            if agent.render:\n",
    "                env.render() #Show cartpole animation\n",
    "\n",
    "            #Get action for the current state and go one step in environment\n",
    "            ###################################\n",
    "#             action = agent.get_action(state)\n",
    "            action_idx = agent.get_action(state)\n",
    "            action = env.action_space[action_idx]\n",
    "            ###################################\n",
    "            next_state, reward, done, _= env.step(action)\n",
    "            \n",
    "            next_state = np.reshape(next_state, [1, state_size]) #Reshape next_state similarly to state\n",
    "\n",
    "            #Save sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            #Training step\n",
    "            agent.train_model()\n",
    "            score += reward #Store episodic reward\n",
    "            state = next_state #Propagate state\n",
    "\n",
    "            if done:\n",
    "                #At the end of very episode, update the target network\n",
    "                if e % agent.target_update_frequency == 0:\n",
    "                    agent.update_target_model()\n",
    "                #Plot the play time for every episode\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "\n",
    "                print(\"episode:\", e, \"  score:\", score,\" q_value:\", max_q_mean[e],\"  memory length:\",len(agent.memory))\n",
    "\n",
    "                # if the mean of scores of last 100 episodes is bigger than 195\n",
    "                # stop training\n",
    "                if agent.check_solve:\n",
    "                    if np.mean(scores[-min(100, len(scores)):]) >= 195:\n",
    "                        print(\"solved after\", e-100, \"episodes\")\n",
    "                        agent.plot_data(episodes,scores,max_q_mean[:e+1])\n",
    "                        sys.exit()\n",
    "    agent.plot_data(episodes,scores,max_q_mean)\n",
    "    # Save the model\n",
    "    agent.save_model(path_to_model, path_to_target)\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "(128, 128, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make('Reacher-v101')\n",
    "state = env.reset()\n",
    "print(state.shape)\n",
    "%matplotlib inline\n",
    "plt.imshow(state)\n",
    "plt.savefig('test.png')\n",
    "plt.show()\n",
    "\n",
    "# images = []\n",
    "# for _ in range(10):\n",
    "#     data = env.render(mode='rgb_array')\n",
    "#     images.append(data)\n",
    "# #     action = env.action_space.sample()\n",
    "# #     actions = [[0.1, 0], [0,0.1]]\n",
    "# #     action = random.choice(actions)\n",
    "#     action = random.choice(env.action_space)\n",
    "# #     print(\"Action: \", action)\n",
    "#     env.step(action)\n",
    "# env.close()\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4bf6770284f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3600x3600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (50,50))\n",
    "for i in range(len(images)):\n",
    "    ax = fig.add_subplot(1, 10, i + 1)\n",
    "    ax.imshow(images[i])\n",
    "plt.show()\n",
    " \n",
    "# plt.imshow(data, interpolation='nearest')\n",
    "\n",
    "# w=10\n",
    "# h=10\n",
    "# fig=plt.figure(figsize=(8, 8))\n",
    "# columns = 5\n",
    "# rows = 2\n",
    "# for i in range(1, columns*rows +1):\n",
    "#     img = np.random.randint(10, size=(h,w))\n",
    "#     fig.add_subplot(rows, columns, i)\n",
    "#     plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
