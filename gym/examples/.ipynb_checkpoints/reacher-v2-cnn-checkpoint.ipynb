{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reacher-v2 Environment with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-0.05, 0],\n",
       " [-0.0001, 0],\n",
       " [0.0001, 0],\n",
       " [0.05, 0],\n",
       " [0, -0.05],\n",
       " [0, -0.0001],\n",
       " [0, 0.0001],\n",
       " [0, 0.05]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('Reacher-v101')\n",
    "state = env.reset()\n",
    "env.observation_space\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-1-de4fc4096c8f>, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-de4fc4096c8f>\"\u001b[0;36m, line \u001b[0;32m62\u001b[0m\n\u001b[0;31m    model.add(Conv2D(64, kernel_size=3, activation=’relu’, input_shape= )\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, load_model\n",
    "import os\n",
    "\n",
    "EPISODES = 1000 #Maximum number of episodes\n",
    "\n",
    "#DQN Agent for the reacher-v2\n",
    "#Q function approximation with NN, experience replay, and target network\n",
    "class DQNAgent:\n",
    "    #Constructor for the agent (invoked when DQN is first called in main)\n",
    "    def __init__(self, state_size, action_space):\n",
    "        self.check_solve = False\t#If True, stop if you satisfy solution condition\n",
    "        self.render = False #If you want to see Cartpole learning, then change to True\n",
    "        self.action_space = action_space\n",
    "        #Get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = len(action_space)\n",
    "        \n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "        #Set hyper parameters for the DQN. Do not adjust those labeled as Fixed.\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.005  # 0.005\n",
    "        self.epsilon = 0.02 #Fixed\n",
    "        self.batch_size = 32 #Fixed\n",
    "        self.memory_size = 500000  # 1000\n",
    "        self.train_start = 1000 #Fixed\n",
    "        self.target_update_frequency = 1\n",
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "        #Number of test states for Q value plots\n",
    "        self.test_state_no =10000 # 10000\n",
    "\n",
    "        #Create memory buffer using deque\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "\n",
    "        #Create main network and target network (using build_model defined below)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        #Initialize target network\n",
    "        self.update_target_model()\n",
    "\n",
    "    #Approximate Q function using Neural Network\n",
    "    #State is the input and the Q Values are the output.\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "        #Edit the Neural Network model here\n",
    "        #Tip: Consult https://keras.io/getting-started/sequential-model-guide/\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(64, kernel_size=3, activation=’relu’, input_shape=state_size )\n",
    "        model.add(Conv2D(32, kernel_size=3, activation=’relu’))\n",
    "        model.add(Conv2D(16, kernel_size=3, activation=’relu’))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4096, activation=’relu’))\n",
    "        model.add(Dense(256, activation=’relu’))\n",
    "        model.add(Dense(self.action_size, activation=’relu’))\n",
    "        #\n",
    "        # self.model = Sequential()\n",
    "        # self.model.add(Dense(16, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        # self.model.add(Dense(16, activation=\"relu\"))\n",
    "        # self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        # self.model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "    #After some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #Get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "        #Insert your e-greedy policy code here\n",
    "        #Tip 1: Use the random package to generate a random action.\n",
    "        #Tip 2: Use keras.model.predict() to compute Q-values from the state.\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action =  random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            action =  np.argmax(q_value[0])\n",
    "        # action = random.randrange(self.action_size)\n",
    "        return action\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "    #Save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) #Add sample to the end of the list\n",
    "\n",
    "    #Sample <s,a,r,s'> from replay memory\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start: #Do not train if not enough memory\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory)) #Train on at most as many samples as you have in memory\n",
    "        mini_batch = random.sample(self.memory, batch_size) #Uniformly sample the memory buffer\n",
    "        #Preallocate network and target network input matrices.\n",
    "        update_input = np.zeros((batch_size, self.state_size)) #batch_size by state_size two-dimensional array (not matrix!)\n",
    "        update_target = np.zeros((batch_size, self.state_size)) #Same as above, but used for the target network\n",
    "        action, reward, done = [], [], [] #Empty arrays that will grow dynamically\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0] #Allocate s(i) to the network input array from iteration i in the batch\n",
    "            action.append(mini_batch[i][1]) #Store a(i)\n",
    "            reward.append(mini_batch[i][2]) #Store r(i)\n",
    "            update_target[i] = mini_batch[i][3] #Allocate s'(i) for the target network array from iteration i in the batch\n",
    "            done.append(mini_batch[i][4])  #Store done(i)\n",
    "\n",
    "        target = self.model.predict(update_input) #Generate target values for training the inner loop network using the network model\n",
    "        target_val = self.target_model.predict(update_target) #Generate the target values for training the outer loop target network\n",
    "        #Q Learning: get maximum Q value at s' from target network\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "        #Insert your Q-learning code here\n",
    "        #Tip 1: Observe that the Q-values are stored in the variable target\n",
    "        #Tip 2: What is the Q-value of the action taken at the last state of the episode?\n",
    "        for i in range(self.batch_size): #For every batch\n",
    "            # target[i][action[i]] = random.randint(0,1)\n",
    "            ############################################################### edited by andy\n",
    "#             action2ind = {(-0.0001, 0):0,(0.0001, 0):1,(0 , -0.0001):2,(0, 0.0001):3}\n",
    "#             action2ind = {(-0.0001, 0):0, (-0.01, 0):1,(0.0001, 0):2, (0.01, 0):3,(0 , -0.0001):4, (0, -0.01):5,(0, 0.0001):6, (0, 0.01):7}\n",
    "            \n",
    "#             action_tuple = tuple(action[i])\n",
    "#             action_ind = action2ind[action_tuple]\n",
    "            action_ind = self.action_space.index(action[i])\n",
    "            if done[i]:\n",
    "                target[i][action_ind]= reward[i]\n",
    "            else:\n",
    "                target[i][action_ind] = reward[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "            #################################################################\n",
    "#             if done[i]:\n",
    "#                 target[i][action[i]]= reward[i]\n",
    "#             else:\n",
    "#                 target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "#                     np.amax(target_val[i]))\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "        #Train the inner loop network\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_model(self, path_to_model, path_to_target):\n",
    "            self.model.save(path_to_model)\n",
    "            self.target_model.save(path_to_target)\n",
    "            return\n",
    "        \n",
    "    def restore_model(self, path_to_model, path_to_target):\n",
    "            self.model = load_model(path_to_model)\n",
    "            self.target_model = load_model(path_to_target)\n",
    "            return\n",
    "        \n",
    "    #Plots the score per episode as well as the maximum q value per episode, averaged over precollected states.\n",
    "    def plot_data(self, episodes, scores, max_q_mean):\n",
    "        pylab.figure(0)\n",
    "        pylab.plot(episodes, max_q_mean, 'b')\n",
    "        pylab.xlabel(\"Episodes\")\n",
    "        pylab.ylabel(\"Average Q Value\")\n",
    "        pylab.savefig(\"qvalues.png\")\n",
    "\n",
    "        pylab.figure(1)\n",
    "        pylab.plot(episodes, scores, 'b')\n",
    "        pylab.xlabel(\"Episodes\")\n",
    "        pylab.ylabel(\"Score\")\n",
    "        pylab.savefig(\"scores.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('Reacher-v101') # Reacher-v101 environment is the edited version of Reacher-v0 adapted for CNN\n",
    "    #Get state and action sizes from the environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    print(\"state size: \", state_size)\n",
    "    action_size = len(env.action_space)\n",
    "#     action_size = env.action_space.n\n",
    "    #Create agent, see the DQNAgent __init__ method for details\n",
    "    agent = DQNAgent(state_size, env.action_space)\n",
    "    \n",
    "    # load the pre-trained model\n",
    "    path_to_model = 'model_cnn.h5'\n",
    "    path_to_target = 'target_model_cnn.h5'\n",
    "    if os.path.isfile(path_to_model) and os.path.isfile(path_to_target):\n",
    "        print(\"Loading the pre-trained model......\")\n",
    "        agent.restore_model(path_to_model, path_to_target)\n",
    "    else:\n",
    "        print(\"Pre-trained model doesn't exist.\")\n",
    "    \n",
    "\n",
    "    # Collect test states for plotting Q values using uniform random policy\n",
    "    test_states = np.zeros((agent.test_state_no, state_size))\n",
    "    max_q = np.zeros((EPISODES, agent.test_state_no))\n",
    "    max_q_mean = np.zeros((EPISODES,1))\n",
    "    \n",
    "    done = True\n",
    "    for i in range(agent.test_state_no):\n",
    "        if done:\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            test_states[i] = state\n",
    "        else:\n",
    "            #############################\n",
    "#             action = random.randrange(action_size)\n",
    "\n",
    "            action_idx = random.randrange(action_size)\n",
    "            action = env.action_space[action_idx]\n",
    "            ###################################\n",
    "#             if done:\n",
    "#                 print(\"Before Done: \", done)\n",
    "            next_state, reward, done, info= env.step(action)\n",
    "#             if done:\n",
    "#                 print(\"Done: \", done)\n",
    "#                 print(\"Info: \", info)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            test_states[i] = state\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    scores, episodes = [], [] #Create dynamically growing score and episode counters\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset() #Initialize/reset the environment\n",
    "        state = np.reshape(state, [1, state_size]) #Reshape state so that to a 1 by state_size two-dimensional array ie. [x_1,x_2] to [[x_1,x_2]]\n",
    "        #Compute Q values for plotting\n",
    "        tmp = agent.model.predict(test_states)  # tmp.shape = 10000 * 4\n",
    "        max_q[e][:] = np.max(tmp, axis=1)\n",
    "        max_q_mean[e] = np.mean(max_q[e][:])\n",
    "        count = 0 \n",
    "        while not done:\n",
    "#             if count % 10 == 0:\n",
    "#                 print(\"counter: \", count)\n",
    "#             count += 1\n",
    "            if agent.render:\n",
    "                env.render() #Show cartpole animation\n",
    "\n",
    "            #Get action for the current state and go one step in environment\n",
    "            ###################################\n",
    "#             action = agent.get_action(state)\n",
    "            action_idx = agent.get_action(state)\n",
    "            action = env.action_space[action_idx]\n",
    "            ###################################\n",
    "            next_state, reward, done, _= env.step(action)\n",
    "            \n",
    "            next_state = np.reshape(next_state, [1, state_size]) #Reshape next_state similarly to state\n",
    "\n",
    "            #Save sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            #Training step\n",
    "            agent.train_model()\n",
    "            score += reward #Store episodic reward\n",
    "            state = next_state #Propagate state\n",
    "\n",
    "            if done:\n",
    "                #At the end of very episode, update the target network\n",
    "                if e % agent.target_update_frequency == 0:\n",
    "                    agent.update_target_model()\n",
    "                #Plot the play time for every episode\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "\n",
    "                print(\"episode:\", e, \"  score:\", score,\" q_value:\", max_q_mean[e],\"  memory length:\",\n",
    "                      len(agent.memory))\n",
    "\n",
    "                # if the mean of scores of last 100 episodes is bigger than 195\n",
    "                # stop training\n",
    "                if agent.check_solve:\n",
    "                    if np.mean(scores[-min(100, len(scores)):]) >= 195:\n",
    "                        print(\"solved after\", e-100, \"episodes\")\n",
    "                        agent.plot_data(episodes,scores,max_q_mean[:e+1])\n",
    "                        sys.exit()\n",
    "    agent.plot_data(episodes,scores,max_q_mean)\n",
    "    # Save the model\n",
    "    agent.save_model(path_to_model, path_to_target)\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating offscreen glfw\n",
      "(256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make('Reacher-v101')\n",
    "state = env.reset()\n",
    "print(state.shape)\n",
    "%matplotlib inline\n",
    "plt.imshow(state)\n",
    "plt.show()\n",
    "\n",
    "# images = []\n",
    "# for _ in range(10):\n",
    "#     data = env.render(mode='rgb_array')\n",
    "#     images.append(data)\n",
    "# #     action = env.action_space.sample()\n",
    "# #     actions = [[0.1, 0], [0,0.1]]\n",
    "# #     action = random.choice(actions)\n",
    "#     action = random.choice(env.action_space)\n",
    "# #     print(\"Action: \", action)\n",
    "#     env.step(action)\n",
    "# env.close()\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4bf6770284f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3600x3600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (50,50))\n",
    "for i in range(len(images)):\n",
    "    ax = fig.add_subplot(1, 10, i + 1)\n",
    "    ax.imshow(images[i])\n",
    "plt.show()\n",
    " \n",
    "# plt.imshow(data, interpolation='nearest')\n",
    "\n",
    "# w=10\n",
    "# h=10\n",
    "# fig=plt.figure(figsize=(8, 8))\n",
    "# columns = 5\n",
    "# rows = 2\n",
    "# for i in range(1, columns*rows +1):\n",
    "#     img = np.random.randint(10, size=(h,w))\n",
    "#     fig.add_subplot(rows, columns, i)\n",
    "#     plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
