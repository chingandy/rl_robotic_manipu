{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "from baselines.common.running_mean_std import RunningMeanStd\n",
    "from envs import *\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from skimage.io import imsave\n",
    "from parser import *\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Storage:\n",
    "    def __init__(self, size, keys=None):\n",
    "        if keys is None:\n",
    "            keys = []\n",
    "        keys = keys + ['s', 'a', 'r', 'm',\n",
    "                       'v', 'q', 'pi', 'log_pi', 'ent',\n",
    "                       'adv', 'ret', 'q_a', 'log_pi_a',\n",
    "                       'mean']\n",
    "        self.keys = keys\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def add(self, data):\n",
    "        for k, v in data.items():\n",
    "            if k not in self.keys:\n",
    "                self.keys.append(k)\n",
    "                setattr(self, k, [])\n",
    "            getattr(self, k).append(v)\n",
    "\n",
    "    def placeholder(self):\n",
    "        for k in self.keys:\n",
    "            v = getattr(self, k)\n",
    "            if len(v) == 0:\n",
    "                setattr(self, k, [None] * self.size)\n",
    "\n",
    "    def reset(self):\n",
    "        for key in self.keys:\n",
    "            setattr(self, key, [])\n",
    "\n",
    "    def cat(self, keys):\n",
    "        data = [getattr(self, k)[:self.size] for k in keys]\n",
    "        #dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "        #data = data.type(dtype)\n",
    "        return map(lambda x: torch.cat(x, dim=0), data)\n",
    "\n",
    "class DummyBody(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(DummyBody, self).__init__()\n",
    "        self.feature_dim = state_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class BaseNet:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "# TODO: look into nn.init.orthogonal_\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "class CategoricalActorCriticNet(nn.Module, BaseNet):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 phi_body=None,\n",
    "                 actor_body=None,\n",
    "                 critic_body=None,\n",
    "                 ordinal_distribution=False):\n",
    "        super(CategoricalActorCriticNet, self).__init__()\n",
    "        if phi_body is None: phi_body = DummyBody(state_dim)\n",
    "        if actor_body is None: actor_body = DummyBody(phi_body.feature_dim)\n",
    "        if critic_body is None: critic_body = DummyBody(phi_body.feature_dim)\n",
    "        self.phi_body = phi_body\n",
    "        self.actor_body = actor_body\n",
    "        self.critic_body = critic_body\n",
    "        self.fc_action = layer_init(nn.Linear(actor_body.feature_dim, action_dim), 1e-3)\n",
    "        self.fc_critic = layer_init(nn.Linear(critic_body.feature_dim, 1), 1e-3)\n",
    "        self.ordinal_distribution = ordinal_distribution\n",
    "\n",
    "        \"\"\" the parameters for the actor and the crititic \"\"\"\n",
    "        self.actor_params = list(self.actor_body.parameters()) + list(self.fc_action.parameters())\n",
    "        self.critic_params = list(self.critic_body.parameters()) + list(self.fc_critic.parameters())\n",
    "        self.phi_params = list(self.phi_body.parameters())\n",
    "        self.to(Config.DEVICE)\n",
    "\n",
    "    def forward(self, obs, action=None):\n",
    "        # change the dimension order numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "        obs = tensor(obs)\n",
    "        if args.observation == 'pixel':\n",
    "            obs = obs.permute(0, 3, 1, 2)\n",
    "        #image_transform = transforms.Compose([\n",
    "        #transforms.ToTensor(),\n",
    "        #])\n",
    "        #obs = image_transform(obs)\n",
    "\n",
    "        phi = self.phi_body(obs)\n",
    "        phi_a = self.actor_body(phi)\n",
    "        phi_v = self.critic_body(phi)\n",
    "        logits = self.fc_action(phi_a)\n",
    "        v = self.fc_critic(phi_v)\n",
    "\n",
    "        if self.ordinal_distribution:\n",
    "            \"\"\" Ordinal Distribution Network \"\"\"\n",
    "            sigmoid = torch.nn.Sigmoid()\n",
    "            logits =  torch.squeeze(logits, 0)\n",
    "            s_i = sigmoid(logits)\n",
    "            one_minus_s = tensor(1) - s_i\n",
    "            _part_1 = torch.log_(s_i)\n",
    "            _part_2 = torch.log_(one_minus_s)\n",
    "            ordinal_logits = [torch.sum(_part_1[:i + 1]).item() + torch.sum(_part_2[i+1:]).item() for i in range(len(s_i))]\n",
    "            # ordinal_logits = [torch.sum(torch.log_(s_i[:i+1])).item() + torch.sum(torch.log_(one_minus_s[i+1:])).item() for i in range(len(s_i))]\n",
    "\n",
    "            ordinal_logits = tensor(ordinal_logits)\n",
    "            ordinal_logits = torch.unsqueeze(ordinal_logits, dim=0)\n",
    "\n",
    "            dist = torch.distributions.Categorical(logits=ordinal_logits)\n",
    "        else:\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).unsqueeze(-1)\n",
    "        entropy = dist.entropy().unsqueeze(-1)\n",
    "        return {'a': action,\n",
    "                'log_pi_a': log_prob,\n",
    "                'ent': entropy,\n",
    "                'v': v}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NatureConvBody(nn.Module):\n",
    "    def __init__(self, in_channels=4):\n",
    "        super(NatureConvBody, self).__init__()\n",
    "        self.feature_dim = 512\n",
    "        self.conv1 = layer_init(nn.Conv2d(in_channels, 32, kernel_size=8, stride=4))\n",
    "        self.conv2 = layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2))\n",
    "        self.conv3 = layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1))\n",
    "        self.fc4 = layer_init(nn.Linear(28 * 28 * 64, self.feature_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.conv1(x))\n",
    "        y = F.relu(self.conv2(y))\n",
    "        y = F.relu(self.conv3(y))\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y = F.relu(self.fc4(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class FCBody(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_units=(64, 64), gate=F.relu):\n",
    "        super(FCBody, self).__init__()\n",
    "        dims = (state_dim,) + hidden_units\n",
    "        self.layers = nn.ModuleList(\n",
    "            [layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.gate = gate\n",
    "        self.feature_dim = dims[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.gate(layer(x))\n",
    "        return x\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = get_logger(tag=config.tag, log_level=config.log_level)\n",
    "        self.task_ind = 0\n",
    "\n",
    "    def close(self):\n",
    "        close_obj(self.task)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.network.state_dict(), '%s.model' % (filename))\n",
    "        with open('%s.stats' % (filename), 'wb') as f:\n",
    "            pickle.dump(self.config.state_normalizer.state_dict(), f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load('%s.model' % filename, map_location=lambda storage, loc: storage)\n",
    "        self.network.load_state_dict(state_dict)\n",
    "        with open('%s.stats' % (filename), 'rb') as f:\n",
    "            self.config.state_normalizer.load_state_dict(pickle.load(f))\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def eval_episode(self):\n",
    "        env = self.config.eval_env\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.eval_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info[0]['episodic_return']\n",
    "            if ret is not None:\n",
    "                break\n",
    "        return ret\n",
    "\n",
    "    def eval_episodes(self):\n",
    "        episodic_returns = []\n",
    "        for ep in range(self.config.eval_episodes):\n",
    "            total_rewards = self.eval_episode()\n",
    "            episodic_returns.append(np.sum(total_rewards))\n",
    "        self.logger.info('steps %d, episodic_return_test %.2f(%.2f)' % (\n",
    "            self.total_steps, np.mean(episodic_returns), np.std(episodic_returns) / np.sqrt(len(episodic_returns))\n",
    "        ))\n",
    "        self.logger.add_scalar('episodic_return_test', np.mean(episodic_returns), self.total_steps)\n",
    "        return {\n",
    "            'episodic_return_test': np.mean(episodic_returns),\n",
    "        }\n",
    "\n",
    "    def record_online_return(self, info, offset=0):\n",
    "        if isinstance(info, dict):\n",
    "            ret = info['episodic_return']\n",
    "            if ret is not None:\n",
    "                self.logger.add_scalar('episodic_return_train', ret, self.total_steps + offset)\n",
    "                self.logger.info('steps %d, episodic_return_train %s' % (self.total_steps + offset, ret))\n",
    "        elif isinstance(info, tuple):\n",
    "            for i, info_ in enumerate(info):\n",
    "                self.record_online_return(info_, i)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def switch_task(self):\n",
    "        config = self.config\n",
    "        if not config.tasks:\n",
    "            return\n",
    "        segs = np.linspace(0, config.max_steps, len(config.tasks) + 1)\n",
    "        if self.total_steps > segs[self.task_ind + 1]:\n",
    "            self.task_ind += 1\n",
    "            self.task = config.tasks[self.task_ind]\n",
    "            self.states = self.task.reset()\n",
    "            self.states = config.state_normalizer(self.states)\n",
    "\n",
    "    def record_episode(self, dir, env):\n",
    "        mkdir(dir)\n",
    "        steps = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            self.record_obs(env, dir, steps)\n",
    "            action = self.record_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info[0]['episodic_return']\n",
    "            steps += 1\n",
    "            if ret is not None:\n",
    "                break\n",
    "\n",
    "    def record_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For DMControl\n",
    "    def record_obs(self, env, dir, steps):\n",
    "        env = env.env.envs[0]\n",
    "        obs = env.render(mode='rgb_array')\n",
    "        imsave('%s/%04d.png' % (dir, steps), obs)\n",
    "\n",
    "\n",
    "class PPOAgent(BaseAgent):\n",
    "    def __init__(self, config):\n",
    "        BaseAgent.__init__(self, config)\n",
    "        self.config = config\n",
    "        self.task = config.task_fn()\n",
    "        self.network = config.network_fn()  # don't put .to(device) here\n",
    "        self.opt = config.optimizer_fn(self.network.parameters())\n",
    "        self.total_steps = 0\n",
    "        self.states = self.task.reset()\n",
    "        self.states = config.state_normalizer(self.states)\n",
    "        self.episodic_returns = []\n",
    "    def step(self):\n",
    "        config = self.config\n",
    "        storage = Storage(config.rollout_length) # initialize the storage\n",
    "        states = self.states # the starting state\n",
    "\n",
    "        \"\"\" Collect a trajectory \"\"\"\n",
    "        for _ in range(config.rollout_length):\n",
    "            prediction = self.network(states)  # a: action, log_pi_a: log_prob, ent: entropy, v: value\n",
    "            next_states, rewards, terminals, info = self.task.step(to_np(prediction['a']))\n",
    "            if info[0]['episodic_return'] is not None:\n",
    "                self.episodic_returns.append(info[0]['episodic_return'])\n",
    "            self.record_online_return(info)\n",
    "            #TODO: check out these normalizers\n",
    "            rewards = config.reward_normalizer(rewards)\n",
    "            next_states = config.state_normalizer(next_states)\n",
    "            storage.add(prediction)\n",
    "            storage.add({'r': tensor(rewards).unsqueeze(-1),\n",
    "                         'm': tensor(1 - terminals).unsqueeze(-1), # to decide where the state is a terminal state\n",
    "                         's': tensor(states)})\n",
    "            states = next_states\n",
    "            self.total_steps += config.num_workers\n",
    "\n",
    "        self.states = states\n",
    "        prediction = self.network(states)\n",
    "        storage.add(prediction)\n",
    "        storage.placeholder()\n",
    "\n",
    "        \"\"\" Calculate the advantage function for each state \"\"\"\n",
    "        advantages = tensor(np.zeros((config.num_workers, 1)))\n",
    "        returns = prediction['v'].detach()\n",
    "        for i in reversed(range(config.rollout_length)):\n",
    "            returns = storage.r[i] + config.discount * storage.m[i] * returns\n",
    "            if not config.use_gae:  # TODO: check out general advantage estimate\n",
    "                advantages = returns - storage.v[i].detach()\n",
    "            else:\n",
    "                td_error = storage.r[i] + config.discount * storage.m[i] * storage.v[i + 1] - storage.v[i]\n",
    "                advantages = advantages * config.gae_tau * config.discount * storage.m[i]+ td_error\n",
    "            storage.adv[i] = advantages.detach() # store the advantage of each state in one rollout\n",
    "            storage.ret[i] = returns.detach() # store the discounted sum of rewards\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = storage.cat(['s', 'a', 'log_pi_a', 'ret', 'adv'])\n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        # advantages = (advantages - advantages.mean()) / advantages.std()  # normalize the advantages => do we really need this? seems like it doesn't help a lot\n",
    "\n",
    "        \"\"\" Update the policy by maximizing the PPO-Clip objective\n",
    "            by config.optimization_epochs epochs\n",
    "        \"\"\"\n",
    "        for _ in range(config.optimization_epochs):\n",
    "\n",
    "            sampler = random_sample(np.arange(states.size(0)), config.mini_batch_size) # create a generator of random indices in batches, states.size: 2048, mini_batch_size: 64\n",
    "            for batch_indices in sampler:\n",
    "                batch_indices = tensor(batch_indices).long()\n",
    "                sampled_states = states[batch_indices]   # this method of slicing only works in pytorch tensors\n",
    "                sampled_actions = actions[batch_indices]\n",
    "                sampled_log_probs_old = log_probs_old[batch_indices]\n",
    "                sampled_returns = returns[batch_indices]\n",
    "                sampled_advantages = advantages[batch_indices]\n",
    "\n",
    "                prediction = self.network(sampled_states, sampled_actions)  # GaussianActorCriticNet\n",
    "                ratio = (prediction['log_pi_a'] - sampled_log_probs_old).exp()\n",
    "                obj = ratio * sampled_advantages\n",
    "                obj_clipped = ratio.clamp(1.0 - self.config.ppo_ratio_clip,\n",
    "                                          1.0 + self.config.ppo_ratio_clip) * sampled_advantages # .clamp() is also a special function in pytorch\n",
    "                policy_loss = -torch.min(obj, obj_clipped).mean() - config.entropy_weight * prediction['ent'].mean()\n",
    "                value_loss = 0.5 * (sampled_returns- prediction['v']).pow(2).mean()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                (policy_loss + value_loss).backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), config.gradient_clip) # do gradient clipping in-place\n",
    "                self.opt.step()\n",
    "\n",
    "def ppo_pixel(**kwargs):\n",
    "    generate_tag(kwargs)\n",
    "    kwargs.setdefault('log_level', 0)\n",
    "    config = Config()\n",
    "    config.merge(kwargs)\n",
    "    config.video_rendering = True\n",
    "    config.task_fn = lambda: Task(config.game, config.video_rendering, num_envs=config.num_workers)\n",
    "    config.eval_env = config.task_fn()\n",
    "    config.num_workers = 1\n",
    "    config.optimizer_fn = lambda params: torch.optim.Adam(params, lr=3e-4, eps=1e-5)\n",
    "    #config.optimizer_fn = lambda params: torch.optim.RMSprop(params, lr=0.00025, alpha=0.99, eps=1e-5)\n",
    "    config.network_fn = lambda: CategoricalActorCriticNet(config.state_dim, config.action_dim, NatureConvBody(in_channels=3))\n",
    "    config.state_normalizer = ImageNormalizer()\n",
    "    config.reward_normalizer = SignNormalizer()\n",
    "    config.discount = 0.99\n",
    "    config.use_gae = True\n",
    "    config.gae_tau = 0.95\n",
    "    config.entropy_weight = 0.01\n",
    "    config.gradient_clip = 0.5\n",
    "    config.rollout_length = 128\n",
    "    config.optimization_epochs = 10\n",
    "    config.mini_batch_size = 64\n",
    "    config.ppo_ratio_clip = 0.2\n",
    "    config.log_interval = 2048\n",
    "    config.max_steps = 1e5\n",
    "    config.save_interval = 10000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    agent = PPOAgent(config)\n",
    "    run_steps(agent)\n",
    "    # plot the episodic returns\n",
    "    import pylab\n",
    "    pylab.figure(0)\n",
    "    pylab.plot(agent.episodic_returns, 'b')\n",
    "    pylab.xlabel(\"Episodes\")\n",
    "    pylab.ylabel(\"Episodic return\")\n",
    "    pylab.savefig(\"pic/ppo_discrete/pixel.png\")\n",
    "    # run_steps(PPOAgent(config))\n",
    "\n",
    "def ppo_feature(**kwargs):\n",
    "    generate_tag(kwargs)\n",
    "    kwargs.setdefault('log_level', 0)\n",
    "    config = Config()\n",
    "    config.merge(kwargs)\n",
    "\n",
    "    config.num_workers = 1 #  originally = 5\n",
    "    config.video_rendering = True\n",
    "    config.task_fn = lambda: Task(config.game, config.video_rendering,num_envs=config.num_workers)\n",
    "    config.eval_env = config.task_fn()\n",
    "    # config.eval_env = Task(config.game)\n",
    "    config.optimizer_fn = lambda params: torch.optim.Adam(params, lr=3e-4, eps=1e-5)\n",
    "    config.network_fn = lambda: CategoricalActorCriticNet(config.state_dim, config.action_dim, FCBody(config.state_dim), ordinal_distribution=False)\n",
    "    config.discount = 0.99\n",
    "    config.use_gae = True\n",
    "    config.gae_tau = 0.95\n",
    "    config.entropy_weight = 0.01\n",
    "    config.gradient_clip = 5\n",
    "    config.rollout_length = 128\n",
    "    config.optimization_epochs = 10\n",
    "    config.mini_batch_size = 32 * 5\n",
    "    config.ppo_ratio_clip = 0.2\n",
    "    config.log_interval = 128 * 5 * 10\n",
    "    # added by chingandy\n",
    "    config.state_normalizer = MeanStdNormalizer()\n",
    "    config.max_steps = 1e5\n",
    "\n",
    "    agent = PPOAgent(config)\n",
    "    run_steps(agent)\n",
    "\n",
    "\n",
    "    # plot the episodic returns\n",
    "    import pylab\n",
    "    pylab.figure(0)\n",
    "    pylab.plot(agent.episodic_returns, 'b')\n",
    "    pylab.xlabel(\"Episodes\")\n",
    "    pylab.ylabel(\"Episodic return\")\n",
    "    pylab.savefig(\"pic/ppo_discrete/cartpole.png\")\n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    mkdir('log')\n",
    "    mkdir('tf_log')\n",
    "    set_one_thread()\n",
    "    random_seed()\n",
    "    select_device(0) # select_device(gpu_id)\n",
    "\n",
    "    env = \"MountainCar-v0\"\n",
    "    ppo_feature(game=env)\n",
    "    \n",
    "#     if args.observation == 'pixel':\n",
    "#         env = \"Reacher-v101\"\n",
    "#         ppo_pixel(game=env)\n",
    "#     elif args.observation == 'feature_n_detector':\n",
    "#         # print(\"argument parser works\")\n",
    "#         env = 'Reacher-v102'\n",
    "#         ppo_feature(game=env)\n",
    "#     elif args.observation == \"cart\":\n",
    "#         env = \"CartPole-v0\"\n",
    "#         ppo_feature(game=env)\n",
    "#     elif args.observation == \"mountain-car\":\n",
    "#         env = \"MountainCar-v0\"\n",
    "#         ppo_feature(game=env)\n",
    "#     else:\n",
    "#         print(\"Observation space isn't specified.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Reacher-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "import os\n",
    "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test episode: 0 R: -41.87823114984594\n",
      "test episode: 1 R: -47.120810941919544\n",
      "test episode: 2 R: -46.45005692415613\n",
      "GLFW error (code %d): %s 65544 b'X11: RandR gamma ramp support seems broken'\n",
      "GLFW error (code %d): %s 65542 b'EGL: Failed to get EGL display: Success'\n",
      "Creating window glfw\n",
      "GLFW error (code %d): %s 65543 b'Requested client API version 1.0, got version 0.0'\n"
     ]
    },
    {
     "ename": "GlfwError",
     "evalue": "Failed to create GLFW window",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGlfwError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-595201092e11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test episode:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'R:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     agent.stop_episode()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/rl_robotic_manipu/deform_manipu/gym/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/rl_robotic_manipu/deform_manipu/gym/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, width, height)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'human'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_viewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/rl_robotic_manipu/deform_manipu/gym/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36m_get_viewer\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'human'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMjViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'depth_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMjRenderContextOffscreen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/.venv/local/lib/python3.5/site-packages/mujoco_py/mjviewer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sim)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ncam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/.venv/local/lib/python3.5/site-packages/mujoco_py/mjviewer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sim)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gui_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/.venv/local/lib/python3.5/site-packages/mujoco_py/mjrendercontext.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.MjRenderContextWindow.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/thesis/.venv/local/lib/python3.5/site-packages/mujoco_py/mjrendercontext.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.MjRenderContext.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/thesis/.venv/local/lib/python3.5/site-packages/mujoco_py/mjrendercontext.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.MjRenderContext._setup_opengl_context\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/thesis/.venv/local/lib/python3.5/site-packages/mujoco_py/opengl_context.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.GlfwContext.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/thesis/.venv/local/lib/python3.5/site-packages/mujoco_py/opengl_context.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.GlfwContext._create_window\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mGlfwError\u001b[0m: Failed to create GLFW window"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for i in range(3):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    R = 0\n",
    "    t = 0\n",
    "    while not done and t < 200:\n",
    "        frames.append(env.render(mode = 'rgb_array'))\n",
    "        action = env.action_space.sample()\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        R += r\n",
    "        t += 1\n",
    "    print('test episode:', i, 'R:', R)\n",
    "#     agent.stop_episode()\n",
    "env.render()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "patch = plt.imshow(frames[0])\n",
    "plt.axis('off')\n",
    "animate = lambda i: patch.set_data(frames[i])\n",
    "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAADn0lEQVR4nO3dsa7TVhgHcF/EM3QqDJ0qVV2YeIcuFUwsPEjUscqDsMPWd2BiqZA6dejlQRiCbq3cOD4mx/6Ov/P7DYhEf4xPPn9x7OM4wwAAAAAAe3A3fvDizxez/+DTH59WW5lza6/Pu+PL2czbw8fvXv5Sa6zP3Xxk2tICtLYBLbW0AC1sQJMdPFWYqA5eY33GBZgqTFQH11qfah1cUoDWNqCllhaghQ1IB3+jgy/QwXWfX4MO/kYHX6CD6z6/BsfBMxwHz3AcXJ4HAAAAAGCR4/GZfB5nw5sdbW/5fTsN72GQha9OP/ndGw+1ZLS95WPdNB88djbUw+FevgVPqizlNNrTIE9/Xt+0e8sHqtPBj4d3fYvuLR+oTgeTXGuHJa3lA1Xo4IuHDVfG3FseAAAA2Moqs0klJ2Z7y0epOZs09VA+ULXJhoetuHBz7i0fpcJb9NT2OzXy3vK719rJ/dbysZ7WWtDhcD++zkG+ESb8mdPaW2Jr+Vg6ODkFBgAAAPjfxbN0s6f6+snHciYruWoFPjv5Lt+IavPBw/Kh9pbfsYcps8K5s97ySRwX3nWmt3wIH7KSU2DKtPaptbX8jj3eFV3fOfWWBwAAALbiHh0b5aO4R8cW+UDVbic8FN/ivrd8LLf03yIfqM5b9NnwZkfbWz6D1qbnWstHMeFPgdY+tbaWD6SDmXNx+y05zOgkDwAAAGzFFR0b5aO4omOLfKCaV3QMw3A43JdfEdFJPpbJhuRq3oRlWL4h95bfq9M4T1+TLRlzb/ndGw+y/AXqJx/IPjg5BU6u5q0MxxcalhxKdpUHAAAA9mXp8X5v+RCu6NgiH6jyFR3Xn+wzH8vP6myU37fW3hJbywcyXUiB1q6gaC2/b629mq3lY3mLZk5rn1dbywMAAAAAAAAAAAAAAPSq2q+u3OKv1z8//P239/9YfkW+fJZcfIHHm//jh5Z/o/gCsyoFTk6Bk1Pg5BQ4OQVOToGTCy7wxaPGioeSe1/+7XRwcgqcnAInp8DJKXBylX8B/PuM51DX+Ai69+XfQgcDAAAAAA1o4rtJUd4dX85m3h4+brAm63EuOrmuO3jKuLN1ME1rYj44Ssk+eO90cHL2wRfYB7Mb9sHJ6eDkZvbBv7/5YZv1YCWTBa5Y2v9++XEYhuefv9Ra4H69+vWnYRg+/P3vZv/jUz2am31wcgqcnAInd2cfnJsOTk6Bk1Pg5BQ4OQVOToGTU+DkFDi5r/CCTwE5labXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210 at 0x7F7D0F541828>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym, PIL\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "array = env.reset()\n",
    "PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
